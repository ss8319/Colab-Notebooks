{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFQWpdHZyctsRROzvEgZnl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ss8319/Colab-Notebooks/blob/main/NEs%2BLeixcon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IW_SZ61XlqMH"
      },
      "outputs": [],
      "source": [
        "!pip install spacy\n",
        "!pip install sil-machine\n",
        "!pip install nltk\n",
        "!pip install xlsxwriter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try with NLTK,Standford NER https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da"
      ],
      "metadata": {
        "id": "0Em7pGjv0era"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "MiQK9DvDmkEi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12b3d63c-e3bd-48ea-ab89-b2e75d4a28a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from machine.tokenization import LatinSentenceTokenizer, LatinWordTokenizer\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import csv\n",
        "import xlsxwriter\n",
        "import spacy # In terminal, python -m spacy download en_core_web_sm\n",
        "#load spacy NER model\n",
        "nlp = spacy.load('en_core_web_sm') #python -m spacy download en_core_web_sm\n",
        "\n",
        "workbook = xlsxwriter.Workbook('spacy_niv11.xlsx')\n",
        "worksheet = workbook.add_worksheet()\n",
        "i=0\n",
        "with open('spacy_niv11.csv','w',encoding='utf-8') as f:\n",
        "    with open('vref.txt' , 'r' , encoding='utf-8') as vrefs , open('en-NIV11.txt' , 'r' , encoding='utf-8') as verses :\n",
        "        header = ['Word', 'Entity','Vref']\n",
        "        writer = csv.writer(f)\n",
        "        # write the header\n",
        "        writer.writerow(header)\n",
        "        for vref , verse in zip(vrefs , verses) :\n",
        "            # Tokenize\n",
        "            sentence_tokenizer = LatinSentenceTokenizer()\n",
        "            sentences = sentence_tokenizer.tokenize(verse)\n",
        "            word_tokenizer = LatinWordTokenizer()\n",
        "            for sentence in sentences:\n",
        "              word_tokenizer.tokenize(sentence)\n",
        "\n",
        "            #word tokenization\n",
        "            ranges = word_tokenizer.tokenize_as_ranges(sentence)\n",
        "            output = \"\"\n",
        "            prev_end = 0\n",
        "            for range in ranges :\n",
        "                #output += f\"[{sentence[prev_end : range.start]}]\" #white space output\n",
        "                #output += f\"[{sentence[range.start : range.end]}]\" #tokens\n",
        "                doc = nlp(sentence[range.start : range.end])\n",
        "                for ent in doc.ents:\n",
        "                    #writer.writerow(['{}'.format(ent.text),'{}'.format(ent.label_),vref])\n",
        "                    #Try XlsxWriter write a string object\n",
        "                    i=i+1 #increase row no\n",
        "                    worksheet.write(i, 0, ent.text)\n",
        "                    worksheet.write(i, 1 , ent.label_)\n",
        "                    worksheet.write(i, 2 , vref)\n",
        "\n",
        "                prev_end = range.end\n",
        "\n",
        "\n",
        "            \n"
      ],
      "metadata": {
        "id": "-f77hm3glyAO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "86a04158-fc5e-43ff-9e05-c061c3128c4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.24.3) or chardet (4.0.0) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-f1046f24bc5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spacy_niv11.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vref.txt'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mvrefs\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en-NIV11.txt'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mverses\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Word'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Entity'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Vref'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'vref.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from machine.tokenization import LatinSentenceTokenizer, LatinWordTokenizer\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import csv\n",
        "#load spacy NER model\n",
        "nlp = spacy.load('en_core_web_sm') #python -m spacy download en_core_web_sm\n",
        "\n",
        "\n",
        "\n",
        "with open('nltkner_niv11.txt','w',encoding='utf-8') as f:\n",
        "    with open('/content/gdrive/MyDrive/vref.txt' , 'r' , encoding='utf-8') as vrefs , open('/content/gdrive/MyDrive/en-NIV11.txt' , 'r' , encoding='utf-8') as verses :\n",
        "        header = ['Word', 'Entity', 'Entity Description', 'Vref']\n",
        "        writer = csv.writer(f)\n",
        "        # write the header\n",
        "        writer.writerow(header)\n",
        "        for vref , verse in zip(vrefs , verses) :\n",
        "            # Tokenize\n",
        "            sentence_tokenizer = LatinSentenceTokenizer()\n",
        "            sentences = sentence_tokenizer.tokenize(verse)\n",
        "            word_tokenizer = LatinWordTokenizer()\n",
        "            #print(\"\\n\".join(\" \".join(word_tokenizer.tokenize(sentence)) for sentence in sentences))\n",
        "            for sentence in sentences:\n",
        "              word_tokenizer.tokenize(sentence)\n",
        "            #word tokenization\n",
        "\n",
        "            ranges = word_tokenizer.tokenize_as_ranges(sentence)\n",
        "            output = \"\"\n",
        "            prev_end = 0\n",
        "            for range in ranges :\n",
        "                output += sentence[prev_end : range.start]\n",
        "                #perform NER on each word/output\n",
        "                doc = nlp(sentence[prev_end : range.start])\n",
        "                for ent in doc.ents :\n",
        "                    #writer.writerow('%s'%{ent.text}, f.write(f'{ent.label_}: {str(spacy.explain(ent.label_))}'), f.write(f'{vref}'))\n",
        "                    #f.write(f'{ent.text} is  {ent.label_}: {str(spacy.explain(ent.label_))} Verse:{verse}\\n')\n",
        "                    f.write(f'{ent.text}\\t')\n",
        "                    f.write(f'{ent.label_}: {str(spacy.explain(ent.label_))}\\t')\n",
        "                    f.write(f'{vref}\\n')\n",
        "                output += f\"[{sentence[range.start : range.end]}]\"\n",
        "                doc = nlp(sentence[range.start : range.end])\n",
        "                for ent in doc.ents :\n",
        "                #     writer.writerow(f.write(f'{ent.text}'), f.write(f'{ent.label_}: {str(spacy.explain(ent.label_))}'), f.write(f'{vref}'))\n",
        "                    f.write(f'{ent.text}\\t')\n",
        "                    f.write(f'{ent.label_}: {str(spacy.explain(ent.label_))}\\t')\n",
        "                    f.write(f'{vref}\\n')\n",
        "                prev_end = range.end"
      ],
      "metadata": {
        "id": "Ab5CiXPV31hK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fjLMxT53mwEq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}